{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Fraud from Enron Email\n",
    "In this project, you will play detective, and put your machine learning skills to use by building an algorithm to identify Enron Employees who may have committed fraud based on the public Enron financial and email dataset.\n",
    "\n",
    "## Section 1\n",
    "* Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. As part of your answer, give some background on the dataset and how it can be used to answer the project question. Were there any outliers in the data when you got it, and how did you handle those?  [relevant rubric items: “data exploration”, “outlier investigation”]\n",
    "\n",
    "Enron Corporation was an American energy, commodities, and services company based in Houston, Texas. [https://en.wikipedia.org/wiki/Enron] Enron employed approximately 20,000 staff and was one of the world's major electricity, natural gas, communications and pulp and paper companies, with claimed revenues of nearly $101 billion during 2000. At the end of 2001, it was revealed that its reported financial condition was sustained by institutionalized, systematic, and creatively planned accounting fraud, known since as the Enron scandal. Enron has since become a well-known example of willful corporate fraud and corruption.\n",
    "\n",
    "The goal of this project is to identify persons of interest (poi), which consist of:\n",
    "* Indicted persons\n",
    "* Reached a settlement or plea deal with the government\n",
    "* Testified in exchange for prosecution immunity\n",
    "\n",
    "Machine learning is an advance tool for classification tasks such as the Enron project. We can recognize patterns and regularities in our data and then make predictions or decisions, through the building of models from the sample inputs.\n",
    "\n",
    "### Enron dataset cleaning and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"./tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "features_list = ['poi',\n",
    "                 'bonus',\n",
    "                 'deferral_payments',\n",
    "                 'deferred_income',\n",
    "                 'director_fees',\n",
    "                 'exercised_stock_options',\n",
    "                 'expenses',\n",
    "                 'from_messages',\n",
    "                 'from_poi_to_this_person',\n",
    "                 'from_this_person_to_poi',\n",
    "                 'loan_advances',\n",
    "                 'long_term_incentive',\n",
    "                 'other',\n",
    "                 'restricted_stock',\n",
    "                 'restricted_stock_deferred',\n",
    "                 'salary',\n",
    "                 'shared_receipt_with_poi',\n",
    "                 'to_messages',\n",
    "                 'total_payments',\n",
    "                 'total_stock_value',\n",
    "                 'fraction_from_poi',\n",
    "                 'fraction_to_poi']\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"rb\") as data_file:\n",
    "    data_dict = pickle.load(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate dataset\n",
    "Load data into a pandas dataframe for easier exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "enron_df = pd.DataFrame.from_dict(data_dict, orient='index')\n",
    "print(enron_df.shape)\n",
    "print(enron_df.dtypes)\n",
    "enron_df.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reformat 'NaN' to numpy nan\n",
    "There are many 'NaN' values which should be reformated to numpy nan and then check again the data types of the Enron dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert NaN to numpy nan\n",
    "enron_df.replace(to_replace='NaN', value=np.nan, inplace=True)\n",
    "\n",
    "print(\"The sum of the nan values: \\n{}\".format(enron_df.isnull().sum()))\n",
    "enron_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The missing values of the financial features are considered zero values and will be replaced later with 0s. As it was noted in the post:\n",
    "\n",
    "https://discussions.udacity.com/t/how-to-start-the-final-project/177617/6\n",
    "\n",
    "the financial feature values were extracted from the \"enron61702insiderpay.pdf\" document. Cells in the table marked as \"-\" are parsed as NaN values in the saved data dictionary, but actually represent values of 0. The conversion of NaNs to 0s is performed when the data is converted from a dictionary to a matrix through the featureFormat() function.\n",
    "\n",
    "However, same does not apply for the email features. Missing values are probably reflective of a true lack of information about the person in the email corpus. Thus, considerations on treatment of email features based on missing values should be different than the treatment of financial features. For the simplicity of the project I will leave them as it is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify the higher salary\n",
    "The bigger salary is the 'TOTAL', which is the sum of all the salaries of the listed Enron employees, so it is definetely an outlier and it will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(enron_df['salary'].idxmax(), enron_df['salary'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = enron_df['poi'].count()\n",
    "pois = enron_df[enron_df['poi'] == True]['poi'].count()\n",
    "print(\"POIs: \", pois)\n",
    "print(\"Non POIs: \", total - pois)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enron employees exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enron_df.index.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's find registries which have no non NAN values (recommended by the reviewer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for employee, nan_values in enron_df.isnull().sum(axis=1).items():\n",
    "    if nan_values > 19:\n",
    "        print(employee, nan_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'LOCKHART EUGENE E' contains only NaN data and it should be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enron_df.loc['LOCKHART EUGENE E']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove outliers\n",
    "The higher salary of 'TOTAL', as well as 'THE TRAVEL AGENCY IN THE PARK', which was revealed from the previous exploration of the list of Enron Employees will be removed, since they don't belong to real persons. 'LOCKHART EUGENE E' will be also removed as it doesn't contain any data. The graphical representation of bonus vs salary depicts some really high values but not unreasonable to pop them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "\n",
    "enron_df.drop(['TOTAL', 'THE TRAVEL AGENCY IN THE PARK', 'LOCKHART EUGENE E'], inplace=True)\n",
    "enron_df.plot.scatter(x = 'salary', y = 'bonus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove outliers from data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Task 2: Remove outliers\n",
    "data_dict.pop('TOTAL')\n",
    "data_dict.pop('THE TRAVEL AGENCY IN THE PARK')\n",
    "data_dict.pop('LOCKHART EUGENE E')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Visualise the data\n",
    "Visualising the data will help to understand our data and find any other outliers. There is a nice representation of the Enron data in the following link:\n",
    "\n",
    "https://public.tableau.com/profile/diego2420#!/vizhome/Udacity/UdacityDashboard\n",
    "\n",
    "This dashboard was made by Diego using Tableau and was posted in the Udacity Discussions Forum:\n",
    "\n",
    "https://discussions.udacity.com/t/enron-data-set-visualization/33340"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enron dataset conclusions\n",
    "It appears that:\n",
    "* There are 21 features\n",
    "    * 1 \"poi\" (person of interest) feature\n",
    "    * 6 email features\n",
    "    * 14 financial features\n",
    "* There are 146 Enron employees\n",
    "    * 128 non POIs\n",
    "    * 18 POIs\n",
    "* All the features have missing 'NaN' values except from 'poi' which is a boolean.\n",
    "* Exploring the data, it was found two features, 'TOTAL' and 'THE TRAVEL AGENCY IN THE PARK', which are not persons and was removed.\n",
    "* Some extreme values which belong to persons are not considered outliers and will help the algorithms to identify pois.\n",
    "* The dataset is susceptible to overfitting since it contains too many features and few data. In the following steps, we will try to apply various techniques to overcome this problem, such as:\n",
    "    * Feature removal\n",
    "    * Feature selection\n",
    "    * Cross-validation\n",
    "\n",
    "Missing values, i.e. 'NaN' strings will be replaced later with 0.0 using the featureFormat() function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Section 2\n",
    "* What features did you end up using in your POI identifier, and what selection process did you use to pick them? Did you have to do any scaling? Why or why not? As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it. (You do not necessarily have to use it in the final analysis, only engineer and test it.) In your feature selection step, if you used an algorithm like a decision tree, please also give the feature importances of the features that you use, and if you used an automated feature selection function like SelectKBest, please report the feature scores and reasons for your choice of parameter values.  [relevant rubric items: “create new features”, “intelligently select features”, “properly scale features”]\n",
    "\n",
    "### Create new features \n",
    "The number of sent/received emails to/from POIs is an interesting feature to be engineered and investigated. A more robust approach is probably the fraction of these emails to all emails than the absolute number of \"poi emails\". This ratio is the messages from/to POIs divided by to/from messages from the employee. In other words, this is a measure of frequency that an employee contacts with POIs. We expect a POI to send/receive emails with other POIs more frequently, hence the higher the ratio the more possible the person to be a POI.\n",
    "1. fraction_to_poi\n",
    "2. fraction_from_poi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Task 3: Create new feature(s)\n",
    "# Create a function that computes either quantity\n",
    "def computeFraction( poi_messages, all_messages ):\n",
    "    ### take care of \"NaN\" when there is no known email address (and so\n",
    "    ### no filled email features), and integer division.\n",
    "    ### in case of poi_messages or all_messages having \"NaN\" value, return 0.\n",
    "    fraction = 0\n",
    "    if poi_messages == \"NaN\" or all_messages == \"NaN\":\n",
    "        return fraction\n",
    "    else:\n",
    "        fraction = float(poi_messages)/float(all_messages)\n",
    "        \n",
    "    return fraction\n",
    "\n",
    "for name in data_dict:\n",
    "\n",
    "    data_point = data_dict[name]\n",
    "\n",
    "    from_poi_to_this_person = data_point[\"from_poi_to_this_person\"]\n",
    "    to_messages = data_point[\"to_messages\"]\n",
    "    fraction_from_poi = computeFraction( from_poi_to_this_person, to_messages )\n",
    "    data_point[\"fraction_from_poi\"] = fraction_from_poi\n",
    "\n",
    "    from_this_person_to_poi = data_point[\"from_this_person_to_poi\"]\n",
    "    from_messages = data_point[\"from_messages\"]\n",
    "    fraction_to_poi = computeFraction( from_this_person_to_poi, from_messages )\n",
    "    data_point[\"fraction_to_poi\"] = fraction_to_poi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select features\n",
    "SelectKBest was used as a univariate feature selection which works by selecting the best features based on univariate statistical tests. SelectKBest removes all but the k highest scoring features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "\n",
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "select = SelectKBest()\n",
    "select.fit(features, labels)\n",
    "scores = select.scores_\n",
    "# Show the scores in a table\n",
    "feature_scores = zip(features_list[1:], scores)\n",
    "ordered_feature_scores = sorted(feature_scores, key=lambda x: x[1], reverse=True)\n",
    "for feature, score in ordered_feature_scores:\n",
    "    print(feature, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The table above shows all the features with their respective scores. The engineered feature \"fraction_to_poi\" is one of the most important features taking the fifth place, behind the \"exercised_stock_options\", \"total_stock_value\", \"bonus\" and \"salary\".\n",
    "* SelectKBest process will be further examined by grid search in a pipeline. Instead of examining the features manually, a grid search of 'k' values in a pipeline will find the optimum 'k' values, which yield better algorithm performances. Numbers of 'k' ranged between 1 and 10, F1 and recall score were chosen as the evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale features\n",
    "The next essential preprocessing step after feature engineering and feature selection is to scale all the features applying the \"MinMaxScaler\" method. Since the range of values of our data varies widely (several orders of magnitude), the machine learning algorithm functions will not work properly without normalization. If a feature has a variance that is orders of magnitude larger than others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected. Therefore, the range of all features should be normalized so that each feature contributes approximately proportionately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "features = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3-6\n",
    "* What algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms?  [relevant rubric item: “pick an algorithm”]\n",
    "* What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well?  How did you tune the parameters of your particular algorithm? What parameters did you tune? (Some algorithms do not have parameters that you need to tune -- if this is the case for the one you picked, identify and briefly explain how you would have done it for the model that was not your final choice or a different model that does utilize parameter tuning, e.g. a decision tree classifier).  [relevant rubric items: “discuss parameter tuning”, “tune the algorithm”]\n",
    "* What is validation, and what’s a classic mistake you can make if you do it wrong? How did you validate your analysis?  [relevant rubric items: “discuss validation”, “validation strategy”]\n",
    "* Give at least 2 evaluation metrics and your average performance for each of them.  Explain an interpretation of your metrics that says something human-understandable about your algorithm’s performance. [relevant rubric item: “usage of evaluation metrics”]\n",
    "\n",
    "### Test algorithms\n",
    "After creating new features, selecting the best features and scaling them, the next natural step is to run the algorithms to see how they perform after these preprocess operations. A useful scikit-learn tool, \"Pipeline\", packages the transformation steps of these operations with the estimation step of an algorithm classifier into a coherent workflow. The reasons to use \"Pipeline\" instead of keeping the steps separate are [https://www.civisanalytics.com/blog/workflows-in-python-using-pipeline-and-gridsearchcv-for-more-compact-and-comprehensive-code/]:\n",
    "\n",
    "1. It makes code more readable (or, if you like, it makes the intent of the code clearer).\n",
    "2. We don’t have to worry about keeping track data during intermediate steps, for example between transforming and estimating.\n",
    "3. It makes it trivial to move ordering of the pipeline pieces, or to swap pieces in and out.\n",
    "4. It allows you to do GridSearchCV on your workflow.\n",
    "\n",
    "### Tune the algorithms\n",
    "Tuning an algorithm is a process in which we optimize the parameters that impact the model in order to enable the algorithm to perform with an improved performance. If we don't tune the algorithms well, performance will be poor with low accuracy, precision or recall. Most of the machine learning algorithms contain a set of parameters (hyperparameters) which should be set up adequately to perform the best. While all of the algorithms attempt to set reasonable default hyperparameters for us, they can often fail to provide optimal results for many real world datasets in practice. To find an optimized combination of hyperparameters, a metric is chosen to measure the algorithm's performance on an independent data set and hyperparameters that maximize this measure are adopted. To make it more concrete through an example, the SVC classifier have several tunable hyperparameters (C, gamma, kernel, etc.) that can greatly affect the model fit. Their values can range greatly. For example C and gamma can be a float, while kernel can take ‘linear’, ‘poly’, ‘rbf’, or ‘sigmoid’ type. Our job is to specify the best combination of C, gamma values and kernel type, that produces optimal performances.\n",
    "\n",
    "Tuning the models is a tedious, time-consuming process and there can sometimes be interactions between the choices we make in one step and the optimal value for a downstream step. Hopefully, there are two simple and easy tuning strategies, grid search and random search. Scikit-learn provides these two methods for algorithm parameter tuning. GridSearchCV allows us to construct a grid of all the combinations of parameters passing one classifier to pipeline each time, tries each combination, and then reports back the best combination. So, instead of trying numerous values for each tuning parameter, GridSearchCV will apply all the combinations of parameters - not just vary them independently - avoiding local optima.\n",
    "\n",
    "To summarize, the power of GridSearchCV is that it multiplies out all the combinations of parameters and tries each one, making a k-fold cross-validated model for each combination. Then, we can ask for predictions and parameters from our GridSearchCV object and it will automatically return to us the best set of predictions, as well as the best parameters.\n",
    "\n",
    "The following algoriths were studied:\n",
    "* Naive Bayes\n",
    "* Support Vector Machines\n",
    "* Decision Trees\n",
    "* K Nearest Neighbors\n",
    "* Random Forest\n",
    "* AdaBoost\n",
    "\n",
    "### Validate analysis\n",
    "The process of measuring the effectiveness of an algorithm for every possible input or how well it is fit and it generalizes to new data is called validation. A common mistake is to train and test the algorithm on the same data, which results to over-fitting, i.e. algorithm performs great on the training dataset, but it suffers on new data. To overcome this, data are separated into training and testing tests. In our analysis two validation steps applied through pipeline:\n",
    "1. Data was separated into 70% training and 30% testing sets.\n",
    "2. Because of the small size of the Enron dataset, stratified shuffle split method was used, which returns multiple stratified randomized splits, thus enhancing cross validation performance. A common problem with small datasets or with datasets that may have a class which dominates the others (in our study case, 88% of the data belongs to non POIs class) is the imbalance in the data. For example, when we split this dataset to 70% of training data, the training set may contain only one class of response variable, i.e. the non POIs (the majority class). To address this problem, stratified shuffle split divides randomly the data into multiple trials. For the investigation of the algorithms, 10 trials were created with 30% test set size. The same split ratio is used in tester.py but with 1000 folds.\n",
    "\n",
    "### Evaluate the algorithms\n",
    "Accuracy, i.e. the fraction of correct predictions is typically not enough information to evaluate a model. Although it is a starting point, it can lead to invalid decisions. Models with high accuracy may have inadequate precision or recall scores. For this reason the evaluation metrics that were assessed are:\n",
    "1. Precision, the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative. The best value is 1 and the worst value is 0. In our study case, precision is when the algorithm guesses that somebody is a POI and actually measures how certain we are that this person really is a POI. For example, a precision of 0.3 means that if the model predicts 100 POIs, the 30 persons are POIs and the rest 70 are non POIs.\n",
    "2. Recall, the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples. The best value is 1 and the worst value is 0. In context to the project, recall shows how well our identifier can pick the POIs. For example, a low recall score of 0.2 indicates that our identifier finds only 20% of all the real POIs in the prediction. The rest 80% of real POIs will not be investigated by the authorities if they trust our model, something that we don't want to happen.\n",
    "3. F1 score,  a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. The formula for the F1 score is: F1 = 2 x (precision x recall) / (precision + recall)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings; warnings.simplefilter('ignore')\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.metrics import classification_report\n",
    "from time import time\n",
    "\n",
    "# Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "\n",
    "# Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "# Instantiate the pipeline steps\n",
    "select = SelectKBest()\n",
    "nb = GaussianNB()\n",
    "svc = SVC()\n",
    "dtc = DecisionTreeClassifier()\n",
    "knc = KNeighborsClassifier()\n",
    "rfc = RandomForestClassifier()\n",
    "abc = AdaBoostClassifier()\n",
    "\n",
    "# Make a dictionary of classifiers\n",
    "classifiers = {\"GaussianNB\": nb, \"SVM\": svc, \"Decision Trees\": dtc, \n",
    "               \"KNN\": knc, \"Random Forest\": rfc, \"AdaBoost\": abc}\n",
    "\n",
    "# Create a function that combines pipeline and grid search and returns the best clf with the best parameters\n",
    "def optimize_clf(clf, param_grid, n_splits):\n",
    "    t0 = time()\n",
    "    # Add pipeline steps into a list\n",
    "    steps = [('feature_selection', select),\n",
    "             ('clf', clf)]\n",
    "    \n",
    "    # Create the pipeline\n",
    "    pipeline = Pipeline(steps)\n",
    "    \n",
    "    # Split data to train/test sets.\n",
    "    features_train, features_test, labels_train, labels_test = \\\n",
    "    train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Create Stratified ShuffleSplit cross-validator.\n",
    "    # Provides train/test indices to split data in train/test sets.\n",
    "    sss = StratifiedShuffleSplit(n_splits=n_splits, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Create grid search\n",
    "    gs = GridSearchCV(pipeline, param_grid, scoring=('f1', 'recall'),\n",
    "                      cv=sss, refit='f1')\n",
    "                \n",
    "    # Fit pipeline on features_train and labels_train\n",
    "    gs.fit(features_train, labels_train)\n",
    "    # Call pipeline.predict() on features_test data to make a set of test predictions\n",
    "    labels_pred = gs.predict(features_test)\n",
    "    # Test predictions using sklearn.classification_report()\n",
    "    report = classification_report(labels_test, labels_pred)\n",
    "    # Find the best parameters and scores\n",
    "    best_params = gs.best_params_\n",
    "    best_score = gs.best_score_\n",
    "    # Print the reports\n",
    "    print(\"Report:\")\n",
    "    print(report)\n",
    "    print(\"Best f1-score:\")\n",
    "    print(best_score)\n",
    "    print(\"Best parameters:\")\n",
    "    print(best_params)\n",
    "    print(\"Time passed: \", round(time() - t0, 3), \"s\")\n",
    "    # Return the best estimator\n",
    "    return gs.best_estimator_\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    print(\"##########################################################################################################\")\n",
    "    print(name)\n",
    "    if clf == nb:\n",
    "        parameters = {'feature_selection__k':list(range(1, 10))}\n",
    "    elif clf == svc:\n",
    "        parameters = [{'feature_selection__k':list(range(1, 10)),\n",
    "                      'clf__C':[10, 100]}]\n",
    "    elif clf == dtc:\n",
    "        parameters = [{'feature_selection__k':list(range(1, 10)),\n",
    "                      'clf__criterion':['gini', 'entropy']}]\n",
    "    elif clf == knc:\n",
    "        parameters = [{'feature_selection__k':list(range(1, 10)),\n",
    "                      'clf__n_neighbors':[3, 5, 7]}]\n",
    "    elif clf == rfc:\n",
    "        parameters = [{'feature_selection__k':list(range(1, 10)),\n",
    "                      'clf__n_estimators':[1, 5, 10]}]\n",
    "    elif clf == abc:\n",
    "        parameters = [{'feature_selection__k':list(range(1, 10)),\n",
    "                      'clf__n_estimators':[45, 50, 55]}]\n",
    "    optimize_clf(clf, parameters, n_splits=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tuning\n",
    "* After testing and tuning the algorithms, most of them perfoms quite well. The best algorithm in terms of acceptable f1 score and reasonable time cost was found to be the decision trees. AdaBoost and Random Forest perfomed equally or better than Decision Trees, but the required processing time is considerably greater. This is an important factor to consider when we want to run multiple times our algorithms. For this reason, decision trees will be further tuned.\n",
    "* In most cases, 'k = 1' gives the best scores. Probably, that shows our algorithm is trained better with fewer features in such a small dataset. To improve algorithm learning ability, most of the features was finally removed and only the best 5 were examined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and Discussion\n",
    "\n",
    "The table below shows the results after training and testing the decision tree algorithm without (first number) and with (second number) the created feature. In most of the cases, it seems that the new engineered feature have a positive impact on the performance. Definitely, they don't affect the performance negatively. Therefore, the algorithm will be further tested with the created features in the list.\n",
    "\n",
    "| k | Accuracy           | Precision           | Recall            |\n",
    "|------------------------|---------------------|-------------------|\n",
    "|10 | 0.80013 / 0.81160  | 0.24696 / 0.28907   | 0.24350 / 0.28300 |\n",
    "| 7 | 0.80380 / 0.81153  | 0.25658 / 0.28133   | 0.24850 / 0.26600 |\n",
    "| 5 | 0.80660 / 0.80947  | 0.27687 / 0.27749   | 0.27950 / 0.26750 |\n",
    "| 3 | 0.81327 / 0.81393  | 0.29130 / 0.30333   | 0.27950 / 0.30500 |\n",
    "| 1 | 0.81973 / 0.81813  | 0.22713 / 0.22000   | 0.14650 / 0.14300 |\n",
    "\n",
    "Finally, after several tests, a smaller list with five features worked better than the whole list of features. As it was mentioned previously, algorithms trained on small datasets with many features tend to overfit during training. Thus, they cannot generalise in new data and their performance is poor. The features included in the list were selected by their scores from the SelectKBest method.\n",
    "\n",
    "Eventually, the model with the optimal set of features and parameters is:\n",
    "\n",
    "### Decision Trees Classifier:\n",
    "\n",
    "* Best parameters:\n",
    "    * 'class_weight': None\n",
    "    * 'criterion': 'gini'\n",
    "    * 'max_features': 'auto'\n",
    "    * 'splitter': 'best'\n",
    "    * 'feature_selection_k': 5\n",
    "\n",
    "\n",
    "* Feature Ranking:\n",
    "    1. feature no. 1: total_stock_value (0.4318449749484233)\n",
    "    2. feature no. 2: salary (0.19978632478632477)\n",
    "    3. feature no. 3: fraction_to_poi (0.16519918459573657)\n",
    "    4. feature no. 4: bonus (0.12090455840455819)\n",
    "    5. feature no. 5: exercised_stock_options (0.08226495726495725)\n",
    "\n",
    "The engineered features 'fraction_to_poi' was selected by SelectKBest method and the optimal 'k' value was found to be 5.\n",
    "\n",
    "* Report generated by tester.py':\n",
    "\n",
    "Pipeline(memory=None,\n",
    "     steps=[('feature_selection', SelectKBest(k=5, score_func=<function f_classif at 0x111207e18>)), ('clf', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
    "            max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
    "            splitter='best'))])\n",
    "\tAccuracy: 0.80571\tPrecision: 0.33349\tRecall: 0.36050\tF1: 0.34647\tF2: 0.35475\n",
    "\tTotal predictions: 14000\tTrue positives:  721\tFalse positives: 1441\tFalse negatives: 1279\tTrue negatives: 10559\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
