{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    Skeleton code for k-means clustering mini-project.\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "\n",
    "def Draw(pred, features, poi, mark_poi=False, name=\"image.png\", f1_name=\"feature 1\", f2_name=\"feature 2\"):\n",
    "    \"\"\" some plotting code designed to help you visualize your clusters \"\"\"\n",
    "\n",
    "    ### plot each cluster with a different color--add more colors for\n",
    "    ### drawing more than five clusters\n",
    "    colors = [\"b\", \"c\", \"k\", \"m\", \"g\"]\n",
    "    for ii, pp in enumerate(pred):\n",
    "        plt.scatter(features[ii][0], features[ii][1], color = colors[pred[ii]])\n",
    "\n",
    "    ### if you like, place red stars over points that are POIs (just for funsies)\n",
    "    if mark_poi:\n",
    "        for ii, pp in enumerate(pred):\n",
    "            if poi[ii]:\n",
    "                plt.scatter(features[ii][0], features[ii][1], color=\"r\", marker=\"*\")\n",
    "    plt.xlabel(f1_name)\n",
    "    plt.ylabel(f2_name)\n",
    "    plt.savefig(name)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "### load in the dict of dicts containing all the data on each person in the dataset\n",
    "data_dict = pickle.load( open(\"final_project_dataset.pkl\", \"rb\") )\n",
    "### there's an outlier--remove it! \n",
    "data_dict.pop(\"TOTAL\", 0)\n",
    "\n",
    "\n",
    "### the input features we want to use \n",
    "### can be any key in the person-level dictionary (salary, director_fees, etc.) \n",
    "feature_1 = \"salary\"\n",
    "feature_2 = \"exercised_stock_options\"\n",
    "poi  = \"poi\"\n",
    "features_list = [poi, feature_1, feature_2]\n",
    "data = featureFormat(data_dict, features_list )\n",
    "poi, finance_features = targetFeatureSplit( data )\n",
    "\n",
    "\n",
    "### in the \"clustering with 3 features\" part of the mini-project,\n",
    "### you'll want to change this line to \n",
    "### for f1, f2, _ in finance_features:\n",
    "### (as it's currently written, the line below assumes 2 features)\n",
    "for f1, f2 in finance_features:\n",
    "    plt.scatter( f1, f2 )\n",
    "plt.show()\n",
    "\n",
    "### cluster here; create predictions of the cluster labels\n",
    "### for the data and store them to a list called pred\n",
    "\n",
    "\n",
    "### rename the \"name\" parameter when you change the number of features\n",
    "### so that the figure gets saved to a different file\n",
    "try:\n",
    "    Draw(pred, finance_features, poi, mark_poi=False, name=\"clusters.pdf\", f1_name=feature_1, f2_name=feature_2)\n",
    "except NameError:\n",
    "    print(\"no predictions object named pred found, no clusters to plot\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What features will your clustering algorithm use?\n",
    "* salary\n",
    "* exercised_stock_options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying Clustering\n",
    "Deploy k-means clustering on the financial_features data, with 2 clusters specified as a parameter. Store your cluster predictions to a list called pred, so that the Draw() command at the bottom of the script works properly. In the scatterplot that pops up, are the clusters what you expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### cluster here; create predictions of the cluster labels\n",
    "### for the data and store them to a list called pred\n",
    "from sklearn.cluster import KMeans\n",
    "classifier = KMeans(n_clusters=2, max_iter=500, n_init=20)\n",
    "classifier.fit(finance_features)\n",
    "pred = classifier.predict(finance_features)\n",
    "\n",
    "### rename the \"name\" parameter when you change the number of features\n",
    "### so that the figure gets saved to a different file\n",
    "try:\n",
    "    Draw(pred, finance_features, poi, mark_poi=False, name=\"clusters.pdf\", f1_name=feature_1, f2_name=feature_2)\n",
    "except NameError:\n",
    "    print(\"no predictions object named pred found, no clusters to plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering with 3 Features\n",
    "Add a third feature to features_list, “total_payments\". Now rerun clustering, using 3 input features instead of 2 (obviously we can still only visualize the original 2 dimensions). Compare the plot with the clusterings to the one you obtained with 2 input features. Do any points switch clusters? How many? This new clustering, using 3 features, couldn’t have been guessed by eye--it was the k-means algorithm that identified it.\n",
    "\n",
    "(You'll need to change the code that makes the scatterplot to accommodate 3 features instead of 2, see the comments in the starter code for instructions on how to do this.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### the input features we want to use \n",
    "### can be any key in the person-level dictionary (salary, director_fees, etc.) \n",
    "feature_1 = \"salary\"\n",
    "feature_2 = \"exercised_stock_options\"\n",
    "feature_3 = \"total_payments\"\n",
    "poi  = \"poi\"\n",
    "features_list = [poi, feature_1, feature_2, feature_3]\n",
    "data = featureFormat(data_dict, features_list )\n",
    "poi, finance_features = targetFeatureSplit( data )\n",
    "\n",
    "\n",
    "### in the \"clustering with 3 features\" part of the mini-project,\n",
    "### you'll want to change this line to \n",
    "### for f1, f2, _ in finance_features:\n",
    "### (as it's currently written, the line below assumes 2 features)\n",
    "for f1, f2, _ in finance_features:\n",
    "    plt.scatter( f1, f2 )\n",
    "plt.show()\n",
    "\n",
    "# KMeans uses a distance metric to determine which points are in which cluster.\n",
    "# By adding a 3rd variable, you are changing the problem from a 2D metric to a 3D metric\n",
    "# (obviously, you are only viewing the solution in 2D, but KMeans is 'clustering' the points in 3D).\n",
    "# _ is a placeholder (it is standard notation used by python programmers for 'I don't care about that variable').\n",
    "# If you plot f.i. over f1 and f3 you see that there is one point far away from all the others,\n",
    "# which explains the 2 clusters you see above.\n",
    "for f1, f2, f3 in finance_features:\n",
    "    plt.scatter( f1, f3 )\n",
    "plt.show()\n",
    "\n",
    "for f1, f2, f3 in finance_features:\n",
    "    plt.scatter( f2, f3 )\n",
    "plt.show()\n",
    "\n",
    "### cluster here; create predictions of the cluster labels\n",
    "### for the data and store them to a list called pred\n",
    "classifier = KMeans(n_clusters=2, max_iter=500, n_init=20)\n",
    "classifier.fit(finance_features)\n",
    "pred = classifier.predict(finance_features)\n",
    "\n",
    "### rename the \"name\" parameter when you change the number of features\n",
    "### so that the figure gets saved to a different file\n",
    "try:\n",
    "    Draw(pred, finance_features, poi, mark_poi=False, name=\"clusters.pdf\", f1_name=feature_1, f2_name=feature_2)\n",
    "except NameError:\n",
    "    print(\"no predictions object named pred found, no clusters to plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When you add another feature, do any points switch clusters?\n",
    "* Yes, four points switch clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stock Option Range\n",
    "In the next lesson, we’ll talk about feature scaling. It’s a type of feature preprocessing that you should perform before some classification and regression tasks. Here’s a sneak preview that should call your attention to the general outline of what feature scaling does.\n",
    "\n",
    "What are the maximum and minimum values taken by the “exercised_stock_options” feature used in this example?\n",
    "\n",
    "(NB: if you look at finance_features, there are some \"NaN\" values that have been cleaned away and replaced with zeroes--so while those might look like the minima, it's a bit deceptive because they're more like points for which we don't have information, and just have to put in a number. So for this question, go back to data_dict and look for the maximum and minimum numbers that show up there, ignoring all the \"NaN\" entries.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the floating-point values of negative infinity and infinity.\n",
    "max_value = float(\"-inf\")\n",
    "min_value = float(\"inf\")\n",
    "\n",
    "for k, v in data_dict.items():\n",
    "    if v[\"exercised_stock_options\"] != \"NaN\":\n",
    "        if v[\"exercised_stock_options\"] > max_value:\n",
    "            max_value = v[\"exercised_stock_options\"]\n",
    "        if v[\"exercised_stock_options\"] < min_value:\n",
    "            min_value = v[\"exercised_stock_options\"]\n",
    "\n",
    "print(\"max_value: \",max_value)\n",
    "print(\"min_value: \",min_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salary Range\n",
    "What are the maximum and minimum values taken by “salary”?\n",
    "\n",
    "(NB: same caveat as in the last quiz. If you look at finance_features, there are some \"NaN\" values that have been cleaned away and replaced with zeroes--so while those might look like the minima, it's a bit deceptive because they're more like points for which we don't have information, and just have to put in a number. So for this question, go back to data_dict and look for the maximum and minimum numbers that show up there, ignoring all the \"NaN\" entries.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the floating-point values of negative infinity and infinity.\n",
    "max_value = float(\"-inf\")\n",
    "min_value = float(\"inf\")\n",
    "\n",
    "for k, v in data_dict.items():\n",
    "    if v[\"salary\"] != \"NaN\":\n",
    "        if v[\"salary\"] > max_value:\n",
    "            max_value = v[\"salary\"]\n",
    "        if v[\"salary\"] < min_value:\n",
    "            min_value = v[\"salary\"]\n",
    "\n",
    "print(\"max_value: \",max_value)\n",
    "print(\"min_value: \",min_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Changes\n",
    "The plot on the next slide shows the exact same clustering code that you just wrote, but in this example we applied feature scaling before performing the clustering.\n",
    "\n",
    "We want you to compare the clustering with scaling (on the next slide) with the first clustering visualization you produced, when you used two features in your clustering algorithm.\n",
    "\n",
    "Notice that now the range of the features has changed to [0.0, 1.0]. That's the only change we've made.\n",
    "\n",
    "In the next lesson you’ll learn a lot more about what feature scaling means, but for now, just look at the effect on the clusters--which point(s) switch their associated cluster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def fit_scaler_on_original_data(data_dict, feature):\n",
    "\n",
    "    feature_not_NaN = []\n",
    "    for person in data_dict.keys():\n",
    "        if data_dict[person][feature] != \"NaN\":\n",
    "            feature_not_NaN.append(data_dict[person][feature])\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(numpy.array(feature_not_NaN, dtype = numpy.float).reshape(-1, 1))\n",
    "    \n",
    "    return scaler\n",
    "\n",
    "feature_1 = \"salary\"\n",
    "feature_2 = \"exercised_stock_options\"\n",
    "poi  = \"poi\"\n",
    "features_list = [poi, feature_1, feature_2]\n",
    "data = featureFormat(data_dict, features_list)\n",
    "\n",
    "# Fit a scaler on the original data (So we don't get deceived by the \"NaN\" points)\n",
    "feature_1_scaler = fit_scaler_on_original_data(data_dict, feature_1)\n",
    "feature_2_scaler = fit_scaler_on_original_data(data_dict, feature_2)\n",
    "\n",
    "# Rescale the data using the appropriate scaler\n",
    "rescaled_data = data\n",
    "rescaled_data[:, 1] = feature_1_scaler.transform(rescaled_data[:, 1].reshape(1, -1))\n",
    "rescaled_data[:, 2] = feature_2_scaler.transform(rescaled_data[:, 2].reshape(1, -1))\n",
    "\n",
    "poi, finance_features = targetFeatureSplit(rescaled_data)\n",
    "\n",
    "classifier = KMeans(n_clusters=2, random_state = 100)\n",
    "pred = classifier.fit_predict(finance_features)\n",
    "\n",
    "try:\n",
    "    Draw(pred, finance_features, poi, mark_poi=False, name=\"clusters_with_feature_scaling.pdf\", f1_name=feature_1, f2_name=feature_2)\n",
    "except NameError:\n",
    "    print(\"no predictions object named pred found, no clusters to plot\")\n",
    "    \n",
    "    \n",
    "print(\"rescaled salary (200000.0) is:\", feature_1_scaler.transform([[200000.0]])[0][0])\n",
    "print(\"rescaled exercised_stock_options (1000000.0) is:\", feature_2_scaler.transform([[1000000.0]])[0][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
